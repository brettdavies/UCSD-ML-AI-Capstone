{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "16ed9cec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/media/bigdaddy/data/cache_huggingface/datasets\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv('../hf.env')\n",
        "HF_TOKEN_PATH = os.getenv(\"HF_TOKEN_PATH\")\n",
        "HF_MODEL_CACHE = os.getenv(\"HF_MODEL_CACHE\")\n",
        "HF_DATASETS_CACHE = os.getenv(\"HF_DATASETS_CACHE\")\n",
        "\n",
        "print(f\"{HF_DATASETS_CACHE}\")\n",
        "\n",
        "# Read the token if it is available\n",
        "try:\n",
        "    with open(HF_TOKEN_PATH, 'r') as token_file:\n",
        "        HF_TOKEN = token_file.read().strip()\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a2787582-554f-44ce-9f38-4180a5ed6b44",
      "metadata": {
        "id": "a2787582-554f-44ce-9f38-4180a5ed6b44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bigdaddy/Documents/GitHub/ucsd-ml-ai-capstone/reproduce-existing-solutions/Fine-Tune-Whisper-For-Multilingual-ASR-with-Transformers/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
            "        num_rows: 6540\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
            "        num_rows: 2894\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Load and prepare the Common Voice dataset for Hindi\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"train+validation\", token=\"use_auth_token\", trust_remote_code=True, keep_in_memory=True)\n",
        "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"test\", token=\"use_auth_token\", trust_remote_code=True, keep_in_memory=True)\n",
        "\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce",
      "metadata": {
        "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['audio', 'sentence'],\n",
            "        num_rows: 6540\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['audio', 'sentence'],\n",
            "        num_rows: 2894\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Remove columns that are not required\n",
        "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
        "\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5",
      "metadata": {
        "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bigdaddy/Documents/GitHub/ucsd-ml-ai-capstone/reproduce-existing-solutions/Fine-Tune-Whisper-For-Multilingual-ASR-with-Transformers/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Whisper feature extractor\n",
        "# The feature extractor preprocesses audio inputs\n",
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "90d056e20b3e4f14ae0199a1a4ab1bb0",
            "d82a88daec0e4f14add691b7b903064c",
            "350acdb0f40e454099fa901e66de55f0",
            "2e6a82a462cc411d90fa1bea4ee60790",
            "c74bfee0198b4817832ea86e8e88d96c",
            "04fb2d81eff646068e10475a08ae42f4"
          ]
        },
        "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
        "outputId": "5c004b44-86e7-4e00-88be-39e0af5eed69"
      },
      "outputs": [],
      "source": [
        "# Initialize the Whisper tokenizer for Hindi\n",
        "# The tokenizer converts text to token ids and vice versa\n",
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6",
      "metadata": {
        "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6"
      },
      "outputs": [],
      "source": [
        "# Combine feature extractor and tokenizer into a single processor\n",
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\", tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255",
      "metadata": {
        "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'audio': {'path': '/media/bigdaddy/data/cache_huggingface/datasets/downloads/extracted/6d6785caa067928defa0cec6ce1c2cdbc932c4cc4b6924427657c234ff0a4274/hi_train_0/common_voice_hi_26008353.mp3', 'array': array([ 5.81611368e-26, -1.48634016e-25, -9.37040538e-26, ...,\n",
            "        1.06425901e-07,  4.46416450e-08,  2.61450239e-09]), 'sampling_rate': 48000}, 'sentence': 'हमने उसका जन्मदिन मनाया।'}\n"
          ]
        }
      ],
      "source": [
        "print(common_voice[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f12e2e57-156f-417b-8cfb-69221cc198e8",
      "metadata": {
        "id": "f12e2e57-156f-417b-8cfb-69221cc198e8"
      },
      "outputs": [],
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "87122d71-289a-466a-afcf-fa354b18946b",
      "metadata": {
        "id": "87122d71-289a-466a-afcf-fa354b18946b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'audio': {'path': '/media/bigdaddy/data/cache_huggingface/datasets/downloads/extracted/6d6785caa067928defa0cec6ce1c2cdbc932c4cc4b6924427657c234ff0a4274/hi_train_0/common_voice_hi_26008353.mp3', 'array': array([ 3.81639165e-17,  2.42861287e-17, -1.73472348e-17, ...,\n",
            "       -1.30981789e-07,  2.63096808e-07,  4.77157300e-08]), 'sampling_rate': 16000}, 'sentence': 'हमने उसका जन्मदिन मनाया।'}\n"
          ]
        }
      ],
      "source": [
        "print(common_voice[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6525c478-8962-4394-a1c4-103c54cce170",
      "metadata": {
        "id": "6525c478-8962-4394-a1c4-103c54cce170"
      },
      "outputs": [],
      "source": [
        "# Define a function to prepare the dataset\n",
        "# This function processes audio data and encodes transcriptions\n",
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b",
      "metadata": {
        "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b"
      },
      "outputs": [],
      "source": [
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2,\n",
        "                                 cache_file_names={\n",
        "                                    \"train\": os.path.join(HF_DATASETS_CACHE, \"mozilla-foundation___common_voice_11_0\",\"hi\",\"train.arrow\"),\n",
        "                                    \"test\": os.path.join(HF_DATASETS_CACHE, \"mozilla-foundation___common_voice_11_0\",\"hi\",\"test.arrow\")\n",
        "                                 }\n",
        "                              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f",
      "metadata": {
        "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "62038ba3-88ed-4fce-84db-338f50dcd04f",
      "metadata": {
        "id": "62038ba3-88ed-4fce-84db-338f50dcd04f"
      },
      "outputs": [],
      "source": [
        "model.generation_config.language = \"hindi\"\n",
        "model.generation_config.task = \"transcribe\"\n",
        "\n",
        "model.generation_config.forced_decoder_ids = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5",
      "metadata": {
        "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "    decoder_start_token_id: int\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
        "    processor=processor,\n",
        "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b22b4011-f31f-4b57-b684-c52332f92890",
      "metadata": {
        "id": "b22b4011-f31f-4b57-b684-c52332f92890"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration, WhisperTokenizer, DataCollatorForSeq2Seq\n",
        "import evaluate\n",
        "\n",
        "# Define the compute metrics function\n",
        "# metric = evaluate.load(\"wer\")\n",
        "metric = evaluate.load(\"wer\", trust_remote_code=True)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions\n",
        "    \n",
        "    # Replace -100 with the pad_token_id in labels\n",
        "    labels[labels == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # Decode the predictions and labels to texts\n",
        "    pred_str = processor.batch_decode(preds, skip_special_tokens=True)\n",
        "    label_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Compute WER\n",
        "    wer = metric.compute(predictions=pred_str, references=label_str)\n",
        "    \n",
        "    # Return the metrics\n",
        "    return {\n",
        "        \"wer\": wer,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e379f4cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AccuracyLoggerCallback initialized\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainerCallback\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class AccuracyLoggerCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.train_accuracies = []\n",
        "        self.eval_accuracies = []\n",
        "        self.epochs = []\n",
        "        print(\"AccuracyLoggerCallback initialized\")\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        print(\"on_log called\")\n",
        "        print(\"on_log Logs type:\", type(logs))  # Debugging print to check log history content\n",
        "        print(\"on_log Logs:\", logs)  # Debugging print to check log history content\n",
        "        print(\"on_log Log History:\", state.log_history)  # Debugging print to check log history content\n",
        "\n",
        "        print(\"on_log state type:\", type(state))  # Debugging print to check log history content\n",
        "        print(\"on_log state:\", state)  # Debugging print to check log history content\n",
        "        print(\"on_log state epocs:\", state.epoch)  # Debugging print to check log history content\n",
        "\n",
        "        print(\"on_log self type:\", type(self))  # Debugging print to check log history content\n",
        "        print(\"on_log self:\", self)  # Debugging print to check log history content\n",
        "        print(\"on_log self epocs:\", self.epochs)  # Debugging print to check log history content\n",
        "\n",
        "        if logs is not None and \"eval_wer\" in logs:\n",
        "            self.epochs.append(state.epoch)\n",
        "            self.eval_accuracies.append(1 - logs[\"eval_wer\"] / 100)  # converting WER to accuracy\n",
        "            print(f\"Eval WER: {logs['eval_wer']}, Accuracy: {1 - logs['eval_wer'] / 100}\")\n",
        "            self.plot_accuracies()\n",
        "\n",
        "        if logs is not None and \"train_wer\" in logs:\n",
        "            self.train_accuracies.append(1 - logs[\"train_wer\"] / 100)  # converting WER to accuracy\n",
        "            print(f\"Train WER: {logs['train_wer']}, Accuracy: {1 - logs['train_wer'] / 100}\")\n",
        "            self.plot_accuracies()\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        print(\"on_epoch_end called\")\n",
        "        self.epochs.append(state.epoch)\n",
        "        \n",
        "        print(\"on_epoch_end state type:\", type(state))  # Debugging print to check log history content\n",
        "        print(\"on_epoch_end state:\", state)  # Debugging print to check log history content\n",
        "        print(\"on_epoch_end state epocs:\", state.epoch)  # Debugging print to check log history content\n",
        "\n",
        "        print(\"on_epoch_end self type:\", type(self))  # Debugging print to check log history content\n",
        "        print(\"on_epoch_end self:\", self)  # Debugging print to check log history content\n",
        "        print(\"on_epoch_end self epocs:\", self.epochs)  # Debugging print to check log history content\n",
        "\n",
        "        # Get training accuracy (for this example, we use WER and convert to accuracy)\n",
        "        if state.log_history:\n",
        "            train_logs = state.log_history[-1]\n",
        "            if \"train_wer\" in train_logs:\n",
        "                train_wer = train_logs[\"train_wer\"]\n",
        "                self.train_accuracies.append(1 - train_wer / 100)  # converting WER to accuracy\n",
        "                print(f\"Train WER Epoch: {state.epoch}, Accuracy: {(1 - train_wer / 100)}\")\n",
        "\n",
        "            # Get evaluation accuracy\n",
        "            if \"eval_wer\" in train_logs:\n",
        "                eval_wer = train_logs[\"eval_wer\"]\n",
        "                self.eval_accuracies.append(1 - eval_wer / 100)  # converting WER to accuracy\n",
        "                print(f\"Eval WER Epoch: {state.epoch}, Accuracy: {(1 - eval_wer / 100)}\")\n",
        "\n",
        "            # Plot the accuracies after each epoch\n",
        "            self.plot_accuracies()\n",
        "        else:\n",
        "            print(\"No log history found at epoch end\")\n",
        "\n",
        "    def plot_accuracies(self):\n",
        "        # clear_output(wait=True)\n",
        "        # Debugging print statements\n",
        "        print(f\"Plotting - Train Epochs: {self.epochs}, Accuracies: {self.train_accuracies}\")\n",
        "        print(f\"Plotting - Eval Epochs: {self.epochs}, Accuracies: {self.eval_accuracies}\")\n",
        "        \n",
        "        train_length_match = len(self.epochs) == len(self.train_accuracies)\n",
        "        eval_length_match = len(self.epochs) == len(self.eval_accuracies)\n",
        "\n",
        "        if train_length_match and eval_length_match:\n",
        "            plt.plot(self.epochs, self.train_accuracies, label='Training Accuracy', marker='o')\n",
        "            plt.plot(self.epochs, self.eval_accuracies, label='Validation Accuracy', marker='x')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.title('Training Accuracy over Epochs')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "            # plt.figure(figsize=(10, 6))\n",
        "        else:\n",
        "            if not train_length_match:\n",
        "                print(f\"Mismatch in lengths - Train Epochs: {len(self.epochs)}, Accuracies: {len(self.train_accuracies)}\")\n",
        "            if not eval_length_match:\n",
        "                print(f\"Mismatch in lengths - Eval Epochs: {len(self.epochs)}, Accuracies: {len(self.eval_accuracies)}\")\n",
        "\n",
        "\n",
        "accuracy_logger = AccuracyLoggerCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a",
      "metadata": {
        "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a"
      },
      "outputs": [],
      "source": [
        "# Define training arguments and configuration\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=f\"{HF_MODEL_CACHE}/whisper-small-hi\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=1000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_dir=f\"{HF_MODEL_CACHE}/whisper-small-hi/logs\",\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d546d7fe-0543-479a-b708-2ebabec19493",
      "metadata": {
        "id": "d546d7fe-0543-479a-b708-2ebabec19493"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bigdaddy/Documents/GitHub/ucsd-ml-ai-capstone/reproduce-existing-solutions/Fine-Tune-Whisper-For-Multilingual-ASR-with-Transformers/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Seq2SeqTrainer with model, datasets, and training configuration\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[accuracy_logger]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "-2zQwMfEOBJq",
      "metadata": {
        "id": "-2zQwMfEOBJq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processor.save_pretrained(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de",
      "metadata": {
        "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bigdaddy/Documents/GitHub/ucsd-ml-ai-capstone/reproduce-existing-solutions/Fine-Tune-Whisper-For-Multilingual-ASR-with-Transformers/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
            "/home/bigdaddy/Documents/GitHub/ucsd-ml-ai-capstone/reproduce-existing-solutions/Fine-Tune-Whisper-For-Multilingual-ASR-with-Transformers/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 30:51, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.082000</td>\n",
              "      <td>0.285844</td>\n",
              "      <td>0.343012</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.061124694376528114, global_step=25, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.15434160128e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.061124694376528114\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.12224938875305623, global_step=50, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.30868320256e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.12224938875305623\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.18337408312958436, global_step=75, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.46302480384e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.18337408312958436\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.24449877750611246, global_step=100, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.61736640512e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.24449877750611246\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.3056234718826406, global_step=125, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=5.7717080064e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.3056234718826406\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.36674816625916873, global_step=150, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=6.92604960768e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.36674816625916873\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.4278728606356968, global_step=175, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=8.08039120896e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.4278728606356968\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.4889975550122249, global_step=200, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=9.23473281024e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.4889975550122249\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.5501222493887531, global_step=225, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.038907441152e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.5501222493887531\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.6112469437652812, global_step=250, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.15434160128e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.6112469437652812\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.6723716381418093, global_step=275, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.269775761408e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.6723716381418093\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.7334963325183375, global_step=300, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.385209921536e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.7334963325183375\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.7946210268948656, global_step=325, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.500644081664e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.7946210268948656\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.8557457212713936, global_step=350, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.616078241792e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.8557457212713936\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.9168704156479217, global_step=375, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.73151240192e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.9168704156479217\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=0.9779951100244498, global_step=400, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.846946562048e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 0.9779951100244498\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: []\n",
            "on_epoch_end called\n",
            "on_epoch_end state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_epoch_end state: TrainerState(epoch=1.0, global_step=409, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.846946562048e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_epoch_end state epocs: 1.0\n",
            "on_epoch_end self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_epoch_end self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_epoch_end self epocs: [1.0]\n",
            "Plotting - Train Epochs: [1.0], Accuracies: []\n",
            "Plotting - Eval Epochs: [1.0], Accuracies: []\n",
            "Mismatch in lengths - Train Epochs: 1, Accuracies: 0\n",
            "Mismatch in lengths - Eval Epochs: 1, Accuracies: 0\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.039119804400978, global_step=425, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.96122638057472e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.039119804400978\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.1002444987775062, global_step=450, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.07666054070272e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.1002444987775062\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.1613691931540342, global_step=475, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.19209470083072e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.1613691931540342\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.2224938875305624, global_step=500, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.30752886095872e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.2224938875305624\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.2836185819070904, global_step=525, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.42296302108672e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.2836185819070904\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.3447432762836184, global_step=550, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.53839718121472e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.3447432762836184\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.4058679706601467, global_step=575, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.65383134134272e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.4058679706601467\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.466992665036675, global_step=600, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.76926550147072e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.466992665036675\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.528117359413203, global_step=625, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.88469966159872e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.528117359413203\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.589242053789731, global_step=650, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.00013382172672e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.589242053789731\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.6503667481662592, global_step=675, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.11556798185472e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.6503667481662592\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.7114914425427874, global_step=700, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.23100214198272e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.7114914425427874\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.7726161369193154, global_step=725, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.34643630211072e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.7726161369193154\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.8337408312958434, global_step=750, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.46187046223872e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.8337408312958434\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.8948655256723717, global_step=775, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.57730462236672e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.8948655256723717\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=1.9559902200488999, global_step=800, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.69273878249472e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 1.9559902200488999\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0]\n",
            "on_epoch_end called\n",
            "on_epoch_end state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_epoch_end state: TrainerState(epoch=2.0, global_step=818, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.69273878249472e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_epoch_end state epocs: 2.0\n",
            "on_epoch_end self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_epoch_end self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_epoch_end self epocs: [1.0, 2.0]\n",
            "Plotting - Train Epochs: [1.0, 2.0], Accuracies: []\n",
            "Plotting - Eval Epochs: [1.0, 2.0], Accuracies: []\n",
            "Mismatch in lengths - Train Epochs: 2, Accuracies: 0\n",
            "Mismatch in lengths - Eval Epochs: 2, Accuracies: 0\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=2.0171149144254277, global_step=825, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.80701860102144e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 2.0171149144254277\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0, 2.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=2.078239608801956, global_step=850, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.92245276114944e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 2.078239608801956\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0, 2.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=2.139364303178484, global_step=875, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.03788692127744e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 2.139364303178484\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0, 2.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=2.2004889975550124, global_step=900, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.15332108140544e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 2.2004889975550124\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0, 2.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=2.26161369193154, global_step=925, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.26875524153344e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 2.26161369193154\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0, 2.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=2.3227383863080684, global_step=950, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.38418940166144e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 2.3227383863080684\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0, 2.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}, {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966, 'step': 975}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=2.3838630806845966, global_step=975, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.49962356178944e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}, {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966, 'step': 975}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 2.3838630806845966\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0, 2.0]\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'loss': 0.082, 'grad_norm': 3.0979137420654297, 'learning_rate': 4e-08, 'epoch': 2.444987775061125}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}, {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966, 'step': 975}, {'loss': 0.082, 'grad_norm': 3.0979137420654297, 'learning_rate': 4e-08, 'epoch': 2.444987775061125, 'step': 1000}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=2.444987775061125, global_step=1000, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.61505772191744e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}, {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966, 'step': 975}, {'loss': 0.082, 'grad_norm': 3.0979137420654297, 'learning_rate': 4e-08, 'epoch': 2.444987775061125, 'step': 1000}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 2.444987775061125\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0, 2.0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50259], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'eval_loss': 0.28584420680999756, 'eval_wer': 0.3430119360027089, 'eval_runtime': 508.6638, 'eval_samples_per_second': 5.689, 'eval_steps_per_second': 0.712, 'epoch': 2.444987775061125}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}, {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966, 'step': 975}, {'loss': 0.082, 'grad_norm': 3.0979137420654297, 'learning_rate': 4e-08, 'epoch': 2.444987775061125, 'step': 1000}, {'eval_loss': 0.28584420680999756, 'eval_wer': 0.3430119360027089, 'eval_runtime': 508.6638, 'eval_samples_per_second': 5.689, 'eval_steps_per_second': 0.712, 'epoch': 2.444987775061125, 'step': 1000}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=2.444987775061125, global_step=1000, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.61505772191744e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}, {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966, 'step': 975}, {'loss': 0.082, 'grad_norm': 3.0979137420654297, 'learning_rate': 4e-08, 'epoch': 2.444987775061125, 'step': 1000}, {'eval_loss': 0.28584420680999756, 'eval_wer': 0.3430119360027089, 'eval_runtime': 508.6638, 'eval_samples_per_second': 5.689, 'eval_steps_per_second': 0.712, 'epoch': 2.444987775061125, 'step': 1000}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 2.444987775061125\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0, 2.0]\n",
            "Eval WER: 0.3430119360027089, Accuracy: 0.9965698806399729\n",
            "Plotting - Train Epochs: [1.0, 2.0, 2.444987775061125], Accuracies: []\n",
            "Plotting - Eval Epochs: [1.0, 2.0, 2.444987775061125], Accuracies: [0.9965698806399729]\n",
            "Mismatch in lengths - Train Epochs: 3, Accuracies: 0\n",
            "Mismatch in lengths - Eval Epochs: 3, Accuracies: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "on_epoch_end called\n",
            "on_epoch_end state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_epoch_end state: TrainerState(epoch=2.444987775061125, global_step=1000, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.61505772191744e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}, {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966, 'step': 975}, {'loss': 0.082, 'grad_norm': 3.0979137420654297, 'learning_rate': 4e-08, 'epoch': 2.444987775061125, 'step': 1000}, {'eval_loss': 0.28584420680999756, 'eval_wer': 0.3430119360027089, 'eval_runtime': 508.6638, 'eval_samples_per_second': 5.689, 'eval_steps_per_second': 0.712, 'epoch': 2.444987775061125, 'step': 1000}], best_metric=0.3430119360027089, best_model_checkpoint='/media/bigdaddy/data/cache_model/whisper-small-hi/checkpoint-1000', is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': True, 'should_epoch_stop': False, 'should_save': True, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_epoch_end state epocs: 2.444987775061125\n",
            "on_epoch_end self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_epoch_end self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_epoch_end self epocs: [1.0, 2.0, 2.444987775061125, 2.444987775061125]\n",
            "Eval WER Epoch: 2.444987775061125, Accuracy: 0.9965698806399729\n",
            "Plotting - Train Epochs: [1.0, 2.0, 2.444987775061125, 2.444987775061125], Accuracies: []\n",
            "Plotting - Eval Epochs: [1.0, 2.0, 2.444987775061125, 2.444987775061125], Accuracies: [0.9965698806399729, 0.9965698806399729]\n",
            "Mismatch in lengths - Train Epochs: 4, Accuracies: 0\n",
            "Mismatch in lengths - Eval Epochs: 4, Accuracies: 2\n",
            "on_log called\n",
            "on_log Logs type: <class 'dict'>\n",
            "on_log Logs: {'train_runtime': 1854.3403, 'train_samples_per_second': 8.628, 'train_steps_per_second': 0.539, 'total_flos': 4.61505772191744e+18, 'train_loss': 0.25986248409748075, 'epoch': 2.444987775061125}\n",
            "on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}, {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966, 'step': 975}, {'loss': 0.082, 'grad_norm': 3.0979137420654297, 'learning_rate': 4e-08, 'epoch': 2.444987775061125, 'step': 1000}, {'eval_loss': 0.28584420680999756, 'eval_wer': 0.3430119360027089, 'eval_runtime': 508.6638, 'eval_samples_per_second': 5.689, 'eval_steps_per_second': 0.712, 'epoch': 2.444987775061125, 'step': 1000}, {'train_runtime': 1854.3403, 'train_samples_per_second': 8.628, 'train_steps_per_second': 0.539, 'total_flos': 4.61505772191744e+18, 'train_loss': 0.25986248409748075, 'epoch': 2.444987775061125, 'step': 1000}]\n",
            "on_log state type: <class 'transformers.trainer_callback.TrainerState'>\n",
            "on_log state: TrainerState(epoch=2.444987775061125, global_step=1000, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.61505772191744e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}, {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966, 'step': 975}, {'loss': 0.082, 'grad_norm': 3.0979137420654297, 'learning_rate': 4e-08, 'epoch': 2.444987775061125, 'step': 1000}, {'eval_loss': 0.28584420680999756, 'eval_wer': 0.3430119360027089, 'eval_runtime': 508.6638, 'eval_samples_per_second': 5.689, 'eval_steps_per_second': 0.712, 'epoch': 2.444987775061125, 'step': 1000}, {'train_runtime': 1854.3403, 'train_samples_per_second': 8.628, 'train_steps_per_second': 0.539, 'total_flos': 4.61505772191744e+18, 'train_loss': 0.25986248409748075, 'epoch': 2.444987775061125, 'step': 1000}], best_metric=0.3430119360027089, best_model_checkpoint='/media/bigdaddy/data/cache_model/whisper-small-hi/checkpoint-1000', is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': True, 'should_epoch_stop': False, 'should_save': True, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
            "on_log state epocs: 2.444987775061125\n",
            "on_log self type: <class '__main__.AccuracyLoggerCallback'>\n",
            "on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>\n",
            "on_log self epocs: [1.0, 2.0, 2.444987775061125, 2.444987775061125]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=0.25986248409748075, metrics={'train_runtime': 1854.3403, 'train_samples_per_second': 8.628, 'train_steps_per_second': 0.539, 'total_flos': 4.61505772191744e+18, 'train_loss': 0.25986248409748075, 'epoch': 2.444987775061125})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Start the training process\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dfc3853",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final plot (in case training stops prematurely)\n",
        "accuracy_logger.plot_accuracies()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c704f91e-241b-48c9-b8e0-f0da396a9663",
      "metadata": {
        "id": "c704f91e-241b-48c9-b8e0-f0da396a9663"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
        "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
        "    \"dataset_args\": \"config: hi, split: test\",\n",
        "    \"language\": \"hi\",\n",
        "    \"model_name\": \"Whisper Small Hi - Sanchit Gandhi\",  # a 'pretty' name for our model\n",
        "    \"finetuned_from\": \"openai/whisper-small\",\n",
        "    \"tasks\": \"automatic-speech-recognition\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06bc39b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Clear out relevant environment variables by setting them to null\n",
        "# os.environ['HF_TOKEN'] = ''\n",
        "# os.environ['HUGGING_FACE_HUB_TOKEN'] = ''\n",
        "\n",
        "# # Import and execute the logout function\n",
        "# from huggingface_hub import logout\n",
        "# huggingface_hub.logout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7a2114e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# Clear model and processor from memory\n",
        "del model\n",
        "del processor\n",
        "\n",
        "# Manually call the garbage collector\n",
        "gc.collect()\n",
        "\n",
        "# Clear CUDA cache if using GPU\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
